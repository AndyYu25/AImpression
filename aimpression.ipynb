{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.datasets.folder import default_loader\nimport torchvision.transforms as tt\nimport torch\nimport torch.nn as nn\nfrom tqdm.notebook import tqdm\nimport torch.nn.functional as F\nfrom torchvision.utils import save_image\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-14T05:13:34.527617Z","iopub.execute_input":"2022-01-14T05:13:34.527891Z","iopub.status.idle":"2022-01-14T05:13:34.534145Z","shell.execute_reply.started":"2022-01-14T05:13:34.527858Z","shell.execute_reply":"2022-01-14T05:13:34.533527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = \"../input/best-artworks-of-all-time\"\n\nimageSize = 128\nbatchSize = 64\nstats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\nlatentSize = 256\nimageRows = 8\nimageCols = 8\nfeatureSize = 64\n\n\ndef get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n\ndevice = get_default_device()\n\nfixedLatent = torch.randn(imageRows * imageCols, latentSize, 1, 1, device=device)\n\ntrain_ds = ImageFolder(DATA_DIR, transform=tt.Compose([ tt.Resize(imageSize),\n                                                        tt.CenterCrop(imageSize),\n                                                        tt.ToTensor(),\n                                                        tt.Normalize(*stats)]))\n\ntrain_dl = DataLoader(train_ds, batchSize, shuffle=True, num_workers=3, pin_memory=True)\n\ngenerator = nn.Sequential(\n            # input is latentSize x 1 x 1\n            nn.ConvTranspose2d(latentSize, featureSize * 16, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(featureSize * 16),\n            nn.ReLU(True),\n            # state size. (featureSize*16) x 4 x 4\n            nn.ConvTranspose2d(featureSize * 16, featureSize * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(featureSize * 8),\n            nn.ReLU(True),\n            # state size. (featureSize*8) x 8 x 8\n            nn.ConvTranspose2d( featureSize * 8, featureSize * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(featureSize * 4),\n            nn.ReLU(True),\n            # state size. (featureSize*4) x 16 x 16\n            nn.ConvTranspose2d( featureSize * 4, featureSize * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(featureSize * 2),\n            nn.ReLU(True),\n            # state size. (featureSize*2) x 32 x 32\n            nn.ConvTranspose2d( featureSize * 2, featureSize, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(featureSize),\n            nn.ReLU(True),\n            # state size. (featureSize) x 64 x 64\n            nn.ConvTranspose2d(featureSize, 3, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. 3 x 128 x 128\n        )\n\n\ndiscriminator = nn.Sequential(\n            # input is 3 x 128 x 128\n            nn.Conv2d(3, featureSize, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (featureSize) x 64 x 64\n            nn.Conv2d(featureSize, featureSize * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(featureSize * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (featureSize * 2) x 32 x 32\n            nn.Conv2d(featureSize * 2, featureSize * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(featureSize * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (featureSize*4) x 16 x 16\n            nn.Conv2d(featureSize * 4, featureSize * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(featureSize * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (featureSize*8) x 8 x 8\n            nn.Conv2d(featureSize * 8, featureSize * 16, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(featureSize * 16),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (featureSize*16) x 4 x 4\n            nn.Conv2d(featureSize * 16, 1, 4, 1, 0, bias=False),\n            nn.Flatten(),\n            nn.Sigmoid()\n        )\n\n\n\ndef denorm(img_tensors):\n    return img_tensors * stats[1][0] + stats[0][0]\n\ndef save_samples(index, latent_tensors, imageRows, imageCols, show=True):\n    fake_images = generator(latent_tensors)\n    fake_fname = 'generated-images-{0:0=4d}.png'.format(index)\n    save_image(denorm(fake_images), fake_fname, nrow=8)\n    print('Saving', fake_fname)\n    if show:\n        fig, ax = plt.subplots(figsize=(imageRows, imageCols))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))\n\ndef train_discriminator(real_images, opt_d):\n    # Clear discriminator gradients\n    opt_d.zero_grad()\n\n    # Pass real images through discriminator\n    real_preds = discriminator(real_images)\n    \n    real_targets = torch.ones(real_images.size(0), 1, device=device)\n    real_loss = F.binary_cross_entropy(real_preds, real_targets)\n    real_score = torch.mean(real_preds).item()\n    \n    # Generate fake images\n    latent = torch.randn(batchSize, latentSize, 1, 1, device=device)\n    fake_images = generator(latent)\n\n    # Pass fake images through discriminator\n    fake_targets = torch.zeros(fake_images.size(0), 1, device=device)\n    fake_preds = discriminator(fake_images)\n    fake_loss = F.binary_cross_entropy(fake_preds, fake_targets)\n    fake_score = torch.mean(fake_preds).item()\n\n    # Update discriminator weights\n    loss = real_loss + fake_loss\n    loss.backward()\n    opt_d.step()\n    return loss.item(), real_score, fake_score\n\ndef train_generator(opt_g):\n    # Clear generator gradients\n    opt_g.zero_grad()\n    \n    # Generate fake images\n    latent = torch.randn(batchSize, latentSize, 1, 1, device=device)\n    fake_images = generator(latent)\n    \n    # Try to fool the discriminator\n    preds = discriminator(fake_images)\n    targets = torch.ones(batchSize, 1, device=device)\n    loss = F.binary_cross_entropy(preds, targets)\n    \n    # Update generator weights\n    loss.backward()\n    opt_g.step()\n    \n    return loss.item()\n\n\ndef fit(epochs, lr, start_idx=1):\n    torch.cuda.empty_cache()\n    \n    # Losses & scores\n    losses_g = []\n    losses_d = []\n    real_scores = []\n    fake_scores = []\n    \n    # Create optimizers\n    opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n    opt_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n    \n    for epoch in range(epochs):\n        for real_images, _ in tqdm(train_dl):\n            # Train discriminator\n            loss_d, real_score, fake_score = train_discriminator(real_images, opt_d)\n            # Train generator\n            loss_g = train_generator(opt_g)\n            \n        # Record losses & scores\n        losses_g.append(loss_g)\n        losses_d.append(loss_d)\n        real_scores.append(real_score)\n        fake_scores.append(fake_score)\n        \n        # Log losses & scores (last batch)\n        print(\"Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(\n            epoch+1, epochs, loss_g, loss_d, real_score, fake_score))\n    \n        # Save generated images\n        save_samples(epoch+start_idx, fixedLatent, imageRows, imageCols, show=False)\n    \n    return losses_g, losses_d, real_scores, fake_scores\n\n\n\nif __name__ == '__main__':\n    lr = 0.0002\n    epochs = 200\n    history = fit(epochs, lr)\n    losses_g, losses_d, real_scores, fake_scores = history","metadata":{"execution":{"iopub.status.busy":"2022-01-14T05:13:34.535928Z","iopub.execute_input":"2022-01-14T05:13:34.536414Z"},"trusted":true},"execution_count":null,"outputs":[]}]}